"""Event definitions and EventPublisherSubscriber implementation."""

from datetime import datetime
from typing import Optional, Dict, Any, Literal, List, Coroutine, Callable
from pydantic import BaseModel, Field
from .models import Message, Step, StepResult # Import core models if needed, or define specific payload structures
import asyncio
from collections import defaultdict
import logging
import structlog # Add structlog import

# Get structlog logger
log = structlog.get_logger(__name__) 

# --- Event Envelope (Structure for all events published/subscribed) ---

class EventEnvelope(BaseModel):
    event_id: str = Field(..., description="ULID/UUID â€“ used for idempotency and identification")
    type: str = Field(..., description="Concrete event type name (e.g., TurnEvent, StepEvent)")
    spec_version: str = Field(..., pattern="^1\\.\\d+\\.\\d+$", description="Semantic version of the payload schema")
    timestamp: datetime = Field(default_factory=datetime.utcnow, description="ISO 8601 timestamp of event creation")
    trace_id: str = Field(..., description="OpenTelemetry trace ID for distributed tracing")
    tenant_id: Optional[str] = Field(None, description="Optional tenant identifier")
    session_id: Optional[str] = Field(None, description="Optional session identifier")
    payload: BaseModel = Field(..., description="Type-specific payload, schema defined below")

# --- Specific Event Payloads ---

class TurnEventPayload(BaseModel):
    """Payload for TurnEvent (Initiation)."""
    turn_id: str = Field(..., description="Unique ID for this specific turn")
    user_message: Message = Field(..., description="The user's input message")
    personality_id: str = Field(..., description="ID of the personality context to use")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Optional key-value pairs for extra context")
    instructions: Optional[str] = Field(None, description="Natural language or structured instruction for the step")
    parameters: Optional[Dict[str, Any]] = Field(None, description="Step-specific parameters (e.g., prompt, tool name, args)")

class StepEventPayload(BaseModel):
    """Payload for StepEvent (Request). Reuses the core Step model."""
    plan_id: str = Field(..., description="ID of the parent plan")
    step_id: str = Field(..., description="Unique ID for this step")
    step_index: int = Field(..., ge=0, description="Sequence position within the plan")
    step_type: Literal["LLM_CALL", "TOOL_CALL", "MEMORY_OP", "EXTERNAL_API"] = Field(..., description="Type of action to perform")
    personality_id: str = Field(..., description="ID of the personality context for this step")
    instructions: str = Field(..., description="Natural language or structured instruction for the step")
    parameters: Dict[str, Any] = Field(default_factory=dict, description="Step-specific parameters (e.g., prompt, tool name, args)")

class StepResultEventPayload(BaseModel):
    """Payload for StepResultEvent (Response). Includes turn/plan context."""
    turn_id: str = Field(..., description="ID of the turn this step belongs to")
    plan_id: str = Field(..., description="ID of the plan this step belongs to")
    step_id: str = Field(..., description="ID of the step that produced this result")
    status: Literal["SUCCEEDED", "FAILED"] = Field(..., description="Final status of the step execution")
    output: Optional[Any] = Field(None, description="Output generated by the step, if successful")
    error: Optional[Dict[str, Any]] = Field(None, description="Error details, if the step failed")
    metrics: Optional[Dict[str, Any]] = Field(None, description="Performance or cost metrics associated with the step")

# New Payloads for Turn Completion/Failure
class TurnCompletedEventPayload(BaseModel):
    """Payload for TurnCompletedEvent."""
    turn_id: str = Field(..., description="ID of the completed turn")
    final_output: Optional[Any] = Field(None, description="The final output/response of the turn") # Simplified for now
    # TODO: Add aggregated metrics if needed
    metrics: Optional[Dict[str, Any]] = Field(None, description="Aggregated metrics for the entire turn")

class TurnFailedEventPayload(BaseModel):
    """Payload for TurnFailedEvent."""
    turn_id: str = Field(..., description="ID of the failed turn")
    error: Dict[str, Any] = Field(..., description="Details about the error that caused the turn to fail")
    # TODO: Add aggregated metrics if needed
    metrics: Optional[Dict[str, Any]] = Field(None, description="Aggregated metrics for the turn before failure")

# --- Placeholder for EventPublisherSubscriber --- #

class EventPublisherSubscriber:
    """Handles in-memory publishing/subscribing to events using asyncio.Queue.
    
    This is a simple implementation for local development and testing.
    It should be replaced with a real message queue integration (e.g., Iggy, Redis) 
    for production or distributed scenarios.
    """

    _queues: Dict[str, asyncio.Queue] = defaultdict(asyncio.Queue)
    _subscriptions: Dict[str, List[asyncio.Queue]] = defaultdict(list)
    _instance = None

    def __new__(cls, *args, **kwargs): # Singleton pattern to ensure one set of queues
        if not cls._instance:
            cls._instance = super(EventPublisherSubscriber, cls).__new__(cls)
            logging.info("Initialized In-Memory EventPublisherSubscriber Singleton.")
        return cls._instance

    def __init__(self):
        # Initialization logic can go here if needed, but queues are defaultdicts
        pass

    async def publish(self, envelope: EventEnvelope):
        """Publishes an event envelope to the appropriate in-memory queue.

        Args:
            envelope: The EventEnvelope to publish.
        """
        event_type = envelope.type
        if event_type not in self._subscriptions:
            # logging.warning(f"No subscriptions for event type '{event_type}'. Event {envelope.event_id} will be dropped.")
            # If we want broadcast even without subscribers, use _queues directly
            # await self._queues[event_type].put(envelope)
            pass # For strict pub/sub, only deliver if subscribed
        else:
            logging.debug(f"Publishing event {envelope.event_id} (type: {event_type}) to {len(self._subscriptions[event_type])} subscribers.")
            # Distribute to all subscribed queues
            for queue in self._subscriptions[event_type]:
                await queue.put(envelope)

    def subscribe(self, event_type: str) -> asyncio.Queue:
        """Subscribes to an event type and returns a queue to receive events.

        Args:
            event_type: The type of event to subscribe to (e.g., 'StepEvent').

        Returns:
            An asyncio.Queue that will receive matching EventEnvelopes.
        """
        logging.info(f"New subscription created for event type: '{event_type}'")
        new_queue = asyncio.Queue()
        self._subscriptions[event_type].append(new_queue)
        return new_queue

    # Optional: Add methods to unsubscribe or get queue status if needed
    def unsubscribe(self, event_type: str, queue: asyncio.Queue):
        """Removes a specific queue subscription.
        
        Args:
            event_type: The event type the queue was subscribed to.
            queue: The specific queue instance to remove.
        """
        if event_type in self._subscriptions:
            try:
                self._subscriptions[event_type].remove(queue)
                logging.info(f"Unsubscribed queue from event type: '{event_type}'")
                if not self._subscriptions[event_type]:
                    del self._subscriptions[event_type] # Clean up empty list
            except ValueError:
                logging.warning(f"Attempted to unsubscribe a queue not found for event type: '{event_type}'")
        else:
            logging.warning(f"Attempted to unsubscribe from non-existent event type subscription: '{event_type}'")

# --- Global Instances --- #
# Create a single shared instance of the publisher/subscriber
event_publisher = EventPublisherSubscriber()

# Global shutdown event for coordinating worker shutdown
shutdown_event = asyncio.Event() 

# --- Worker Functions (moved from server.py) --- #
async def step_event_worker(
    publisher: EventPublisherSubscriber, 
    processor: 'StepProcessor', # String hint for StepProcessor
    shutdown_event_flag: asyncio.Event
):
    log.info("step_event_worker started")
    subscriber_queue = publisher.subscribe('StepEvent') # Use correct event type string
    try:
        while not shutdown_event_flag.is_set():
            try:
                event_envelope: EventEnvelope = await asyncio.wait_for(subscriber_queue.get(), timeout=1.0)
                if isinstance(event_envelope.payload, StepEventPayload):
                    log.info(f"StepEventWorker received step event for turn {event_envelope.payload.turn_id}, step {event_envelope.payload.step_id}")
                    try:
                        await processor.execute_step(event_envelope.payload)
                    except Exception as e:
                        log.error(f"Error processing step {event_envelope.payload.step_id} in StepEventWorker: {e}", exc_info=True)
                else:
                    log.warning(f"StepEventWorker received non-StepEventPayload on 'StepEvent' channel: {type(event_envelope.payload)}")
                subscriber_queue.task_done()
            except asyncio.TimeoutError:
                continue # Allow checking shutdown_event periodically
            except Exception as e:
                log.exception(f"Unexpected error in step_event_worker loop: {e}")
                await asyncio.sleep(1) # Avoid fast spinning on persistent errors
    finally:
        log.info("step_event_worker shutting down")
        publisher.unsubscribe('StepEvent', subscriber_queue)

async def step_result_event_worker(
    publisher: EventPublisherSubscriber, 
    manager: 'TurnManager', # String hint for TurnManager
    shutdown_event_flag: asyncio.Event
):
    log.info("step_result_event_worker started")
    subscriber_queue = publisher.subscribe('StepResultEvent') # Use correct event type string
    try:
        while not shutdown_event_flag.is_set():
            try:
                event_envelope: EventEnvelope = await asyncio.wait_for(subscriber_queue.get(), timeout=1.0)
                if isinstance(event_envelope.payload, StepResultEventPayload):
                    log.info(f"StepResultEventWorker received step result event for turn {event_envelope.payload.turn_id}, step {event_envelope.payload.step_id}")
                    try:
                        await manager.handle_step_result_event(event_envelope)
                    except Exception as e:
                        log.error(f"Error processing step result for turn {event_envelope.payload.turn_id} in StepResultEventWorker: {e}", exc_info=True)
                else:
                    log.warning(f"StepResultEventWorker received non-StepResultEventPayload on 'StepResultEvent' channel: {type(event_envelope.payload)}")
                subscriber_queue.task_done()
            except asyncio.TimeoutError:
                continue # Allow checking shutdown_event periodically
            except Exception as e:
                log.exception(f"Unexpected error in step_result_event_worker loop: {e}")
                await asyncio.sleep(1) # Avoid fast spinning on persistent errors
    finally:
        log.info("step_result_event_worker shutting down")
        publisher.unsubscribe('StepResultEvent', subscriber_queue)

# --- Worker Management Functions (moved and adapted from server.py) --- #
def start_event_workers(
    publisher: EventPublisherSubscriber,
    turn_manager: 'TurnManager', # String hint
    step_processor: 'StepProcessor', # String hint
    shutdown_event_flag: asyncio.Event,
    num_step_event_workers: int = 2,
    num_step_result_event_workers: int = 2 # Renamed for clarity
) -> List[asyncio.Task]:
    """Creates and starts the background event worker tasks."""
    tasks = []
    for i in range(num_step_event_workers):
        task = asyncio.create_task(step_event_worker(publisher, step_processor, shutdown_event_flag), name=f"step-event-worker-{i}")
        tasks.append(task)
    log.info(f"Started {num_step_event_workers} step_event_worker tasks.")

    for i in range(num_step_result_event_workers):
        task = asyncio.create_task(step_result_event_worker(publisher, turn_manager, shutdown_event_flag), name=f"step-result-event-worker-{i}")
        tasks.append(task)
    log.info(f"Started {num_step_result_event_workers} step_result_event_worker tasks.")
    
    return tasks

async def stop_event_workers(tasks: List[asyncio.Task], shutdown_event_flag: asyncio.Event):
    """Signals and waits for worker tasks to complete."""
    if not tasks:
        log.info("No worker tasks to stop.")
        return

    log.info(f"Stopping {len(tasks)} event worker tasks...")
    shutdown_event_flag.set() # Signal all workers to shut down

    # Wait for all tasks to complete
    # Give a grace period for tasks to finish
    done, pending = await asyncio.wait(tasks, timeout=10.0) # Example timeout

    for task in done:
        try:
            await task # Propagate exceptions if any occurred during shutdown
            log.info(f"Worker task {task.get_name()} completed gracefully.")
        except asyncio.CancelledError:
            log.info(f"Worker task {task.get_name()} was cancelled during shutdown.")
        except Exception as e:
            log.error(f"Worker task {task.get_name()} raised an exception during shutdown: {e}", exc_info=True)

    if pending:
        log.warning(f"{len(pending)} worker tasks did not complete within timeout. Forcing cancellation.")
        for task in pending:
            task.cancel()
            try:
                await task # Allow cancellation to propagate
            except asyncio.CancelledError:
                log.info(f"Worker task {task.get_name()} was forcefully cancelled.")
            except Exception as e:
                # Log any errors that occur even during forced cancellation
                log.error(f"Error during forced cancellation of task {task.get_name()}: {e}", exc_info=True)
    
    log.info("All event worker tasks have been processed for shutdown.") 